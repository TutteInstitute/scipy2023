{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c9c51c3-8d38-42c2-97db-158916fea344",
   "metadata": {},
   "source": [
    "All text annotations are temporary, and for guiding John."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6508a1-2485-49a0-910f-3f60fc97502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.io\n",
    "from collections import defaultdict\n",
    "from dask.distributed import LocalCluster, Client, as_completed\n",
    "import gzip\n",
    "from hashlib import md5\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "from pathlib import Path\n",
    "import scipy.sparse as ss\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import sys\n",
    "import thisnotthat as tnt\n",
    "from tqdm.auto import tqdm\n",
    "import umap\n",
    "import vectorizers as vz\n",
    "import vectorizers.transformers as vzt\n",
    "import zstandard as zstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88b19b-386f-4f43-af57-7988acc7bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.output_notebook()\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f485889-a8d6-4486-a0f3-14e1085ff7c7",
   "metadata": {},
   "source": [
    "Dask makes things go zzzzzoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eca4c1-a9f4-4dd6-ba49-f7c476f6d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bac1c3-7e99-4262-ac59-96f675692808",
   "metadata": {},
   "source": [
    "We will work on host 501, processing all days. One can also choose days between 18 and 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e614f-15da-4531-a3ef-596bdf55a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = 501\n",
    "DAYS = [\"*\"]\n",
    "HOSTNAME = f\"SysClient{HOST:04d}.systemia.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f6be7-1d2a-4752-9f20-1bc8bc77ac52",
   "metadata": {},
   "source": [
    "The data lives as compressed JSON-lines chunks, Zstd-compressed. The following data engineering goes much less deep into token generation than the work we presented so far, so as to put the emphasis on the vectorization (not the data engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3e552-ddb2-4ed6-a7a6-f598a7e9f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = {\n",
    "    \"FLOW\": [\"object\", \"action\", (\"src_ip\", \"ip\"), (\"dest_ip\", \"ip\"), (\"src_port\", \"port\"), (\"dest_port\", \"port\"), \"l4protocol\", \"direction\"],\n",
    "    \"FILE\": [\"object\", \"action\", (\"file_path\", \"path\"), \"info_class\", (\"new_path\", \"path\")],\n",
    "    \"HOST\": [\"object\", \"action\"],\n",
    "    \"MODULE\": [\"object\", \"action\", (\"module_path\", \"path\")],\n",
    "    \"REGISTRY\": [\"object\", \"action\", (\"key\", \"registry-key\"), (\"value\", \"registry-value\"), (\"type\", \"registry-type\")],\n",
    "    \"SERVICE\": [\"object\", \"action\", (\"name\", \"service-name\")],\n",
    "    \"SHELL\": [\"object\", \"action\"],\n",
    "    \"TASK\": [\"object\", \"action\", \"path\", (\"task_name\", \"task-name\")],\n",
    "    \"THREAD\": [\"object\", \"action\"],\n",
    "    \"USER_SESSION\": [\"object\", \"action\", (\"user\", \"user-domain\"), (\"requesting_domain\", \"domain\"), (\"requesting_user\", \"user\"), (\"src_ip\", \"ip\"), (\"src_port\", \"port\")],\n",
    "    \"PROCESS\": {\n",
    "        \"CREATE\": {\n",
    "            \"actorID\": [\"object\", \"action\", (\"image_path\", \"child\"), (\"image_path\", \"path\")],\n",
    "            \"objectID\": [(\"parent_image_path\", \"parent\"), (\"image_path\", \"process\"), (\"user\", \"user-domain\")]\n",
    "        },\n",
    "        \"OPEN\": {\n",
    "            \"actorID\": [\"object\", \"action\"]\n",
    "        },\n",
    "        \"TERMINATE\": {\n",
    "            \"actorID\": [\"object\", \"action\"]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d68368-9fec-4256-a7e9-7ccf558218c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_events(path_chunk):\n",
    "    with zstd.open(path_chunk, mode=\"rt\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip ill-formed records.\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf9de4-8938-47b2-b66c-428f3dafb952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(path_chunk):\n",
    "    for event in iter_events(path_chunk):\n",
    "        obj = event[\"object\"]\n",
    "        if obj not in schemas:\n",
    "            continue\n",
    "        schema = (\n",
    "            schemas[obj].get(event[\"action\"], {})\n",
    "            if isinstance(schemas[obj], dict)\n",
    "            else {\"actorID\": schemas[obj]}\n",
    "        )\n",
    "        for identifier, features in schema.items():\n",
    "            tokens = []\n",
    "            for feature in features:\n",
    "                field, kind = (feature, feature) if isinstance(feature, str) else feature\n",
    "                if value := event.get(field, \"\"):\n",
    "                    tokens.append((kind, value))\n",
    "            yield (pd.Timestamp(event[\"timestamp\"]), event[identifier], tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9668846d-1895-4bf1-b7fb-1e6fe6d103d2",
   "metadata": {},
   "source": [
    "Given a data chunk, we return a data frame where its categorical tokens are already in the list form suitable for one-hot vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19459d00-01ed-4e82-8f8a-b66dad70f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabulate_features(path_chunk):\n",
    "    return pd.DataFrame(\n",
    "        data=extract_features(path_chunk),\n",
    "        columns=[\"timestamp\", \"process_id\", \"tokens\"]\n",
    "    ).astype({\"process_id\": \"category\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002d84f-838c-4b91-b737-8b5beef2bff3",
   "metadata": {},
   "source": [
    "Where are my data chunks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f402e9e4-9f18-4c1d-8785-4d496ef347df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_HOSTNAME = Path(\"/data/optc/scipy2023\") / HOSTNAME\n",
    "CHUNKS = sorted(sum(\n",
    "    [list(ROOT_HOSTNAME.glob(f\"{day}/optc-eng.*.json.zstd\")) for day in DAYS],\n",
    "    []\n",
    "))\n",
    "len(CHUNKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bececc9-399b-4eb5-91ea-4c0424072eec",
   "metadata": {},
   "source": [
    "The vectorization gambit is to do it by chunks, and combine the resulting sparse matrices afterwards, using nifty NgramVectorizer addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3fb560-8114-416c-bfc2-d422d2ed472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_features(path_chunk):\n",
    "    return vz.NgramVectorizer().fit(tabulate_features(path_chunk)[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3bbd38-92f5-491e-ad93-c16c8c91fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vzr_all = sum(\n",
    "    (fut.result() for fut in tqdm(client.map(vectorize_features, CHUNKS), total=len(CHUNKS))),\n",
    "    vz.NgramVectorizer()\n",
    ")\n",
    "event_matrix = vzr_all._train_matrix\n",
    "event_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11fc4cd-96b7-436e-be07-80941c14c1c3",
   "metadata": {},
   "source": [
    "Let's now group events by process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6242f3-d59a-4d6c-b639-73450269138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_by_process(path_chunk):\n",
    "    process_ids = tabulate_features(path_chunk)[[\"process_id\"]]\n",
    "    process_ids[\"event_index\"] = pd.Series(process_ids.index).apply(lambda x: [x])\n",
    "    return process_ids.groupby(\"process_id\", as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d064f52-44a6-46da-86bd-56db4afddaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "process2ievent = {}\n",
    "total_events = 0\n",
    "for fut in tqdm(client.map(events_by_process, CHUNKS), total=len(CHUNKS)):\n",
    "    processes = fut.result()\n",
    "    for process_id, indices in processes[[\"process_id\", \"event_index\"]].itertuples(index=False):\n",
    "        process2ievent.setdefault(process_id, [])\n",
    "        for index_row_chunk in indices:\n",
    "            process2ievent[process_id].append(index_row_chunk + total_events)\n",
    "    total_events += processes[\"event_index\"].apply(len).sum()\n",
    "\n",
    "len(process2ievent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd99162-a160-41d5-9bb2-4f44919a2ad5",
   "metadata": {},
   "source": [
    "That's a *lot* of processes. Let's prune off those for which we don't have enough features (by weight) to reliably describe their behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c74f8-e846-451e-b8cb-2bfcdbe3b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_per_event = np.array(event_matrix.sum(axis=1)).squeeze()\n",
    "features_per_process = pd.Series({process_id: sum([features_per_event[i] for i in indices]) for process_id, indices in tqdm(process2ievent.items())})\n",
    "features_per_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87428c49-b604-4624-98a4-0d5eaaf3296a",
   "metadata": {},
   "source": [
    "Distribution of number of features per process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0024d20-620e-47ff-b22a-64a07c846304",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_per_process.apply(np.log10).hist(bins=range(-1, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1cd962-583e-4c21-8aaa-78735e6bb9a1",
   "metadata": {},
   "source": [
    "It does not make much sense to me to keep processes described by a total number of categorical features less than 10. So let's drop the guys from the first column.\n",
    "\n",
    "We will do that while also putting together process vectors by summing event vectors.\n",
    "This means a linear combination of the rows of the event matrix.\n",
    "The fastest way of achieving that is by computing a projection matrix that we will multiply on the left of the event matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831928a2-25f4-4c06-88ad-aef607ef3257",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "irows = []\n",
    "icols = []\n",
    "process2irow = {}\n",
    "irow2process = {}\n",
    "irow_next = 0\n",
    "for process_id, indices in tqdm(process2ievent.items()):\n",
    "    if features_per_process.loc[process_id] >= 10:\n",
    "        irow = irow_next\n",
    "        irow_next += 1\n",
    "        irows += [irow] * len(indices)\n",
    "        icols += indices\n",
    "        process2irow[process_id] = irow\n",
    "        irow2process[irow] = process_id\n",
    "\n",
    "projection = ss.coo_matrix((np.ones((len(irows),), dtype=np.int32), (irows, icols)), shape=(len(process2irow), event_matrix.shape[0])).tocsr()\n",
    "assert set(np.array(projection.sum(axis=0)).squeeze()) <= {0, 1}\n",
    "projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e2689-fbaf-440f-ba77-aae450f31514",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_matrix = (projection @ event_matrix).astype(np.float32)\n",
    "process_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775a87d-ffd3-43de-8aac-eab06b8e53a4",
   "metadata": {},
   "source": [
    "The categories (_labels_) for our process instances are either the command line by which they were started, or when we can't find that, their related image path.\n",
    "The former can only be found in `PROCESS-CREATE` events.\n",
    "The latter is field common to all events, and its value should be shared by all events generated by any given process instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1a285-bc90-4354-ba43-a6ee89b79dd5",
   "metadata": {},
   "source": [
    "The way we associate labels to process instances is thus to extract the best label we can from every event.\n",
    "We then tabulate these in association with their process ID, and use an *importance* ordinal to denote which label should take precedence.\n",
    "We sort this table by importance, and drop process ID duplicates: what remains are the best guest we can take as label for every process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6b555c-78da-4a44-8643-a285c8ffcefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_labels(proposals):\n",
    "    return proposals.sort_values(\"importance\", ascending=True).drop_duplicates(subset=[\"process_id\"], keep=\"first\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9736c95-b448-4a1e-9787-1cf3a75922f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_processes(path_chunk):\n",
    "    data = []\n",
    "    for event in iter_events(path_chunk):\n",
    "        if event[\"object\"] == \"PROCESS\" and event[\"action\"] == \"CREATE\":\n",
    "            if command_line := event.get(\"command_line\", \"\"):\n",
    "                data.append((event[\"objectID\"], 0, command_line))\n",
    "            elif image_path := event.get(\"image_path\", \"\"):\n",
    "                data.append((event[\"objectID\"], 10, image_path))\n",
    "            if parent_image_path := event.get(\"parent_image_path\", \"\"):\n",
    "                data.append((event[\"actorID\"], 10, parent_image_path))\n",
    "        else:\n",
    "            if image_path := event.get(\"image_path\", \"\"):\n",
    "                data.append((event[\"actorID\"], 10, image_path))\n",
    "\n",
    "    return filter_labels(pd.DataFrame(data=data, columns=[\"process_id\", \"importance\", \"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6e75dc-c612-453f-900c-b4660e800730",
   "metadata": {},
   "source": [
    "We then run this filtering iteratively across best proposals from every chunk, and come out the other end with every process instance labeled... or nearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f505ad-4c31-4fb0-83c5-d3232e40b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_known = pd.DataFrame()\n",
    "for fut in tqdm(client.map(label_processes, CHUNKS), total=len(CHUNKS)):\n",
    "    labels_known = filter_labels(pd.concat([labels_known, fut.result()], ignore_index=True))\n",
    "labels_known"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4c2a4-d02a-4980-9169-008a2072ed17",
   "metadata": {},
   "source": [
    "Any process missing a label, now, we just consider we **don't know** what they are about.\n",
    "Let's bin these together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f5f2a-83ce-438f-9f0a-3f720f4230cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.Series(irow2process, name=\"process_id\").to_frame().merge(labels_known[[\"process_id\", \"label\"]], on=\"process_id\", how=\"left\").fillna(\"(unknown)\")\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84ecb16-4b39-4d6e-beb7-498b150fbeae",
   "metadata": {},
   "source": [
    "Now, not all features are _useful_ for characterizing the process instances.\n",
    "*Orphan features* are too few for their sharing to denote similarity between more than a very small group of processes.\n",
    "*Spurious features* are too often associated to processes to help differentiate between them (like stop words).\n",
    "So a quick thresholding might help compress our very large feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1835f399-528a-42c5-b4f8-57578a583c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.Series(np.array(process_matrix.sum(axis=0)).squeeze())\n",
    "sum(feature_importance == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d93937-640d-475c-b72e-020bccd7eb86",
   "metadata": {},
   "source": [
    "So, the pruning of the set of processes already leaves 41 features completely useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d88cdd-b3d6-40bb-85fc-c6402f261234",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.loc[feature_importance > 0].apply(np.log10).hist(bins=[-2,-1,0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7241dc-210e-449f-be4c-8166a44fceab",
   "metadata": {},
   "source": [
    "Most features, by a large factor, are orphans; we seem not to have any spurious feature, as none is associated to more than 10000 process instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd8993-c4bd-461f-bda8-9d947df300be",
   "metadata": {},
   "source": [
    "Let's take a more detailed look at the first column of the previous histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf2283a-a87e-4d3e-b014-3e47091bbdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.loc[feature_importance < 10].hist(bins=np.linspace(0, 10, 10) - 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a013c-98ff-4a92-afb1-833fa05a0b4e",
   "metadata": {},
   "source": [
    "Again, most of these rarely used features are literal orphans: associated to one or two processes.\n",
    "Let's cut off any that's not tied to at least 3 processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae0219-a0eb-4664-84f6-cdc1486729fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "col2token = []\n",
    "token2col = {}\n",
    "indices_keep = []\n",
    "for i, count in enumerate(feature_importance):\n",
    "    if count > 3:\n",
    "        indices_keep.append(i)\n",
    "        token = vzr_all.column_index_dictionary_[i]\n",
    "        index_new = len(col2token)\n",
    "        col2token.append(token)\n",
    "        token2col[token] = index_new\n",
    "\n",
    "reduced_matrix = process_matrix[:, indices_keep].copy()\n",
    "reduced_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68043021-80bf-4382-b961-9c97d49285ad",
   "metadata": {},
   "source": [
    "Ok, has this feature space reduction killed the representation of processes?\n",
    "I'm hoping the total feature weight for any process is at least 5 (e.g. 5 tokens associated to it across all events that characterize it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf327c-746a-442f-9e94-0ddc3463a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_per_process_redux = np.array(reduced_matrix.sum(axis=1)).squeeze()\n",
    "assert np.min(features_per_process_redux) > 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46f115-3a86-4ad5-805e-e301f39f6041",
   "metadata": {},
   "source": [
    "Now, it's always easier to compute the compressed vector representation on the subset of unique process vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fcac53-cbe8-400c-ad8d-03e0c9cdd014",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def md5_list(it):\n",
    "    return struct.unpack(\"<QQ\", md5(memoryview(np.array(it))).digest())\n",
    "\n",
    "reduced_lil = reduced_matrix.tolil()\n",
    "hh = np.zeros(shape=(reduced_matrix.shape[0], 4), dtype=np.uint64)\n",
    "for i, indices_values in enumerate(zip(reduced_lil.rows, reduced_lil.data)):\n",
    "    hh[i, :] = sum((md5_list(it) for it in indices_values), ())\n",
    "_, index_u, inverse_u, counts_u = np.unique(hh, axis=0, return_index=True, return_inverse=True, return_counts=True)\n",
    "index_u.shape, inverse_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4df7f3-aa2a-4491-b998-2b1a8775dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_matrix = reduced_matrix[index_u, :]\n",
    "unique_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f02e10-1e36-4a23-97a8-dd6806385a77",
   "metadata": {},
   "source": [
    "I have tried running the information weight transform on the matrix of unique process vectors,\n",
    "but the result seems to confuse UMAP **a lot**.\n",
    "UMAP would crash on that matrix by putting way too many vectors under one particular leaf of the RP tree:\n",
    "Leland mused that the hyperplanes used to spread the vectors between the RP trees were doing a poor job.\n",
    "I didn't have the time to truly debug this, so I moved on with directly compressing the matrix of unique process vectors."
   ]
  },
  {
   "cell_type": "raw",
   "id": "84d058a3-1af8-45a5-b177-ae4d425b0d46",
   "metadata": {},
   "source": [
    "%%time\n",
    "normalized_matrix = Normalizer(norm=\"l1\").fit_transform(unique_matrix)\n",
    "normalized_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5f526ce-75e8-48df-9979-4ad37de2a1e1",
   "metadata": {},
   "source": [
    "%%time\n",
    "infoweight_matrix = vzt.InformationWeightTransformer().fit_transform(normalized_matrix).astype(np.float32)\n",
    "infoweight_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432da390-3cf1-46d3-b622-c1c343977b69",
   "metadata": {},
   "source": [
    "The protomap only contains the unique vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb2911-1da4-4e86-bd8b-04839b85269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "process_protomap = umap.UMAP(n_components=2, metric=\"cosine\", densmap=True, dens_lambda=4, n_epochs=800, verbose=True).fit_transform(Normalizer(norm=\"l1\").fit_transform(unique_matrix))\n",
    "process_protomap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1927e5-a4b8-40cc-b796-e1dbbecbb09e",
   "metadata": {},
   "source": [
    "The full map is the protomap reduplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2b20b-0c6f-4808-b2d4-f0ed23511a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_map = process_protomap[inverse_u, :]\n",
    "process_map.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806eae98-4842-4a9d-afc9-c8a516b53cdc",
   "metadata": {},
   "source": [
    "The following will visualize a map of all process instances, where we color the most frequent process classes (top 12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce371c-1e82-4d5e-a21c-dddb45384c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processes_top12 = labels.groupby(\"label\", as_index=False).agg({\"process_id\": \"count\"}).sort_values(\"process_id\", ascending=False).head(12)\n",
    "processes_top12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c706d95-3b34-4eb3-998e-c0fcb3acf6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(processes_top12[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4419aa84-5f82-4f52-b2c6-5ac1407c8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_map_study = process_map[labels.loc[labels[\"label\"] == 'C:\\\\Windows\\\\SYSTEM32\\\\cmd.exe /c \"C:\\\\ncr\\\\DeleteArchiveSecurity.bat\"'].index, :]\n",
    "process_map_study.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa76a4-2a0b-4fc0-9f06-07e998befad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = tnt.BokehPlotPane(process_map_study, width=900, height=900, show_legend=False)\n",
    "editor = tnt.LabelEditorWidget([])\n",
    "editor.link_to_plot(plot)\n",
    "summary = tnt.DataSummaryPane()\n",
    "pn.Row(plot, editor, summary)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a5eaba2-2580-4ace-973d-e2bcfccb7092",
   "metadata": {},
   "source": [
    "labels_top12_ = set(processes_top12[\"label\"])\n",
    "labels_top12 = labels[[\"process_id\"]].copy()\n",
    "labels_top12[\"label\"] = labels[\"label\"].apply(lambda lb: lb if lb in labels_top12_ else \"(other)\")\n",
    "labels_top12"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91804d1e-045a-4dd2-ad4c-73a56a8a07c4",
   "metadata": {},
   "source": [
    "labels_top12_only = labels_top12.loc[labels_top12[\"label\"] != \"(other)\"]\n",
    "process_map_top12_only = process_map[labels_top12_only.index, :]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b62e27bb-40c4-4b96-948f-5b7d9bac6c12",
   "metadata": {},
   "source": [
    "plot = tnt.BokehPlotPane(process_map_top12_only, labels=labels_top12_only[\"label\"], width=900, height=900, show_legend=False)\n",
    "editor = tnt.LabelEditorWidget(plot.labels)\n",
    "editor.link_to_plot(plot)\n",
    "pn.Row(plot, editor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d497f1-d546-46f3-a879-5f08acb68292",
   "metadata": {},
   "source": [
    "Ready for demos of exploring the map contents using summarizers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scipy 2023",
   "language": "python",
   "name": "scipy2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
