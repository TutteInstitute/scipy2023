{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceec70d5-5426-48f8-9f4d-8b26b55800c6",
   "metadata": {},
   "source": [
    "**Commonplace detection in categorical telemetry data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef24d3c-1df9-4cc9-8af6-b4ace622fabe",
   "metadata": {},
   "source": [
    "# Interactive visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fef7d9e-4119-4617-b3e5-686cc9cd300c",
   "metadata": {},
   "source": [
    "This notebook puts up an experimental visualization dashboard to enable the study of the behavior of the software processes for which [notebook 01](01%20Data%20Engineering.ipynb) computed a representation\n",
    "(canned to cloud storage for the impatient).\n",
    "This representation emphasizes the feature correlations between process instances: two process vectors are similar when they are characterized by many common categorical features; otherwise they are not similar, and their mutual distance is larger.\n",
    "Thus, groups of similar process instances make up clusters that show up clearly in scatter plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c7a537-ad4d-4081-8be8-61f7244abf25",
   "metadata": {},
   "source": [
    "We use here the [ThisNotThat](https://thisnotthat.readthedocs.io/en/latest/) data mapping framework,\n",
    "which leverages the [Panel](https://panel.holoviz.org/) dashboard toolkit,\n",
    "to display and annotate such scatter plots interactively.\n",
    "These annotations of clusters augments the initial labeling of process instances using [command lines](01%20Data%20Engineering.ipynb#command-line),\n",
    "yielding novel sets of categorical labels that document observed software behavior.\n",
    "These labels are stored in a Pandas data frame,\n",
    "enabling their further use in the development of ad hoc analytics.\n",
    "In addition to examining the correlations between processes,\n",
    "selection of clusters by lassoing performs on-the-fly feature importance analysis experiments, showing how a clump of points is characterized compared to the rest.\n",
    "Such experiments provide an upper bound on the performance of a classifier one may want to develop to detect the phenomenon expressed through the selected points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84d65a-d68b-4e69-b486-79463f5c44e1",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d3f8d-ecfa-4a4d-81d4-8c5da6daec91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bokeh.io\n",
    "import bokeh.plotting as bpl\n",
    "import cloudpickle as cpkl\n",
    "import fsspec\n",
    "import gzip\n",
    "import itertools as it\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "from pathlib import Path\n",
    "import scipy.sparse as ss\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm.auto import tqdm\n",
    "import thisnotthat as tnt\n",
    "from vectorizers.transformers import CategoricalColumnTransformer, InformationWeightTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b2cc85-0cf2-4b75-9d10-51705bf5a047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bokeh.io.output_notebook()\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362441d-97ae-455c-aa16-75450a56170d",
   "metadata": {},
   "source": [
    "## Gather the data map and its metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b16e5-2496-4227-8fea-7ff059b1bb1a",
   "metadata": {},
   "source": [
    "If one has computed [their own vector representation](01%20Data%20Engineering.ipynb) of the data,\n",
    "local files are used.\n",
    "Otherwise, we grab a vector representation that I computed and stored in an Azure Blob Container\n",
    "(accessible without authentication).\n",
    "If one would use the my own vectors instead of those they computed,\n",
    "they can rename local file `manifest.json` to, say, `manifest.bak`.\n",
    "Or delete any/all of the files listed in `files_vectors` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da83779-4b52-44ee-b1b9-decacfb098a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files_vectors = [\"manifest.json\", \"features.npz\", \"map2d.npz\", \"metadata.csv.gz\", \"labels.csv.gz\", \"col2token.pkl.gz\"]\n",
    "if all([Path(f).is_file() for f in files_vectors]):\n",
    "    print(\"Using process map and metadata stored LOCALLY.\")\n",
    "    FS = fsspec.filesystem(\"file\")\n",
    "    ROOT = \".\"\n",
    "else:\n",
    "    print(\"Using CANNED process map and metadata (from Azure container).\")\n",
    "    FS = fsspec.filesystem(\"abfs\", account_name=\"scipy2023\")\n",
    "    ROOT = \"optc/map\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc2ec33-e989-4bd6-b400-3faafdf5e737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with FS.open(f\"{ROOT}/manifest.json\", \"rt\", encoding=\"utf-8\") as file:\n",
    "    manifest = json.load(file)\n",
    "HOST = manifest[\"host\"]\n",
    "DAYS = manifest[\"days\"]\n",
    "print(f\"Context: host {HOST}; days {', '.join(DAYS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6d194-f207-48ef-86b6-7feaeb6b0fb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with FS.open(f\"{ROOT}/features.npz\", \"rb\") as file_features:\n",
    "    features = ss.load_npz(file_features)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b187ae-2ff0-4106-b784-20468e01ae0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with (\n",
    "    FS.open(f\"{ROOT}/col2token.pkl.gz\", \"rb\") as file_compressed,\n",
    "    gzip.open(file_compressed, \"rb\") as file_pkl\n",
    "):\n",
    "    col2token = dict(enumerate(cpkl.load(file_pkl)))\n",
    "\n",
    "assert len(col2token) == features.shape[1]\n",
    "for i, (k, v) in enumerate(col2token.items()):\n",
    "    if i >= 25:\n",
    "        break\n",
    "    print(k, \":\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3139c083-87f7-40c3-8d9a-3cdbf456cfbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with FS.open(f\"{ROOT}/metadata.csv.gz\", \"rb\") as file_metadata:\n",
    "    metadata = pd.read_csv(file_metadata, parse_dates=[\"timestamp\"], compression=\"gzip\")\n",
    "assert metadata.shape[0] == features.shape[0]\n",
    "metadata[\"timestamp\"] = metadata[\"timestamp\"].apply(pd.Timestamp)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109ff15-65d7-46f7-bcc0-66abc98b427f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with FS.open(f\"{ROOT}/labels.csv.gz\", \"rb\") as file_labels:\n",
    "    labels = pd.read_csv(file_labels, compression=\"gzip\")\n",
    "assert labels.shape[0] == features.shape[0]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0936f5-bda8-4ea1-8b1d-d489e4b832f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with FS.open(f\"{ROOT}/map2d.npz\", \"rb\") as file_vectors:\n",
    "    process_map = np.load(file_vectors)[\"process_map\"]\n",
    "assert process_map.shape == (features.shape[0], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9194f204-84c5-4b03-a1a4-795d75627af8",
   "metadata": {},
   "source": [
    "## Restricting the study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b7921f-c7c7-45a7-ba60-48e8039cf233",
   "metadata": {},
   "source": [
    "If one cares to look,\n",
    "one discovers that most processes in the dataset belong to a handful of dominating classes.\n",
    "Let's scope this experiment to the top-15 process classes with the most associated instances.\n",
    "The reader is welcome to look at other classes as curiosity strikes them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c83c7-1dd0-494a-99fa-f0c26396389e",
   "metadata": {},
   "source": [
    "First, let's capture that top-15 of categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7882212-16bc-4542-929b-1f30897be147",
   "metadata": {},
   "outputs": [],
   "source": [
    "top15_labels = labels.groupby(\"label\", as_index=False).agg({\"process_id\": \"count\"}).sort_values(\"process_id\", ascending=False).head(15)\n",
    "top15_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98acc98f-95bb-435b-a27e-5665c568d110",
   "metadata": {},
   "source": [
    "Now, let's restrict the vectors and metadata to instances belonging to these 15 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7914b6-84e3-4be4-997d-d0d119b19e42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels_top15 = labels.loc[labels[\"label\"].isin(set(top15_labels[\"label\"]))].copy()\n",
    "indices_top15 = labels_top15.index.copy()\n",
    "labels_top15.reset_index(drop=True, inplace=True)\n",
    "labels_top15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb22a2-2dbf-4e96-90e4-8af7faef020d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processes_top15 = process_map[indices_top15, :]\n",
    "features_top15 = features[indices_top15, :]\n",
    "metadata_top15 = metadata.loc[indices_top15].copy()\n",
    "processes_top15.shape, features_top15.shape, metadata_top15.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2376ac06-f25a-4ff0-9d81-792e28ec5d20",
   "metadata": {},
   "source": [
    "## See you lazer, summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1100f024-8f91-40bd-96f4-955f1c4be52f",
   "metadata": {},
   "source": [
    "A key tool brought forward by [ThisNotThat](https://thisnotthat.readthedocs.io/en/latest/) are *interactive summarizers*: data frames or plots rendered as some kind of summary of data points selected in an associated scatter plot,\n",
    "on the fly.\n",
    "TNT offers a good few summarizers out of the box,\n",
    "but we need two more here to support the work of understanding cyber telemetry.\n",
    "The first bespoke summarizer computes the features with the best joint support for the selected points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c497d40-2b67-448c-98da-23a64f8886a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseSupportSummarizer:\n",
    "    \"\"\"\n",
    "    Summarizer for a DataSummaryPane.\n",
    "    This takes a sparse matrix of counts or importances.  Then for any selection of data it computes the\n",
    "    column marginals of that matrix and finds the columns with the largest marginals.\n",
    "\n",
    "    It returns a DataFrame with the top max_features features along with their column marginals and support.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    matrix: a sparse matrix\n",
    "        This is the matrix which we will use for computing the marginals\n",
    "    column_index_dictionary: dict\n",
    "         A dictionary mapping from column indices to column names\n",
    "    max_features: int <default: 10>\n",
    "        The number of features to return\n",
    "    proportional_support: bool <default: True>\n",
    "        Should the proportion be normalized (True) or left as a raw count (False)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        matrix,\n",
    "        column_index_dictionary,\n",
    "        max_features= 10,\n",
    "        proportional_support = True\n",
    "    ):\n",
    "        self.matrix = matrix\n",
    "        self.column_index_dictionary = column_index_dictionary\n",
    "        self.max_features = max_features\n",
    "        self.proportional_support = proportional_support\n",
    "\n",
    "    def summarize(self, selected):\n",
    "        data = self.matrix[plot.selected,:]\n",
    "        column_marginal = np.array(data.sum(axis=0)).squeeze()\n",
    "        largest_indices = np.argsort(column_marginal)[::-1][:self.max_features]\n",
    "        features = [self.column_index_dictionary[x] for x in largest_indices]\n",
    "        kinds, values = zip(*features)\n",
    "        importance = column_marginal[largest_indices]\n",
    "        support = np.sort(np.array((data>0).sum(axis=0)).squeeze())[::-1][:self.max_features]\n",
    "        if self.proportional_support:\n",
    "            support = support / data.shape[0]\n",
    "        return pd.DataFrame({'Kind': kinds, 'Value': values, 'Total weight':importance, 'support':support})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52573a98-0aec-4598-82f6-4f751b4d5c08",
   "metadata": {},
   "source": [
    "The second summarizer encoded here is a copy of one offered by TNT for computing feature importance by training a one-vs-all classifier of the selected data against the rest.\n",
    "TNT's version does not handle sparse feature matrices yet, so we had to reimplement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c769c79-1c3b-4733-a0cd-10c707cfc084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseFeatureImportanceSummarizer:\n",
    "    \"\"\"\n",
    "    Summarizer for the PlotSummaryPane that constructs a class balanced, L1 penalized,\n",
    "    logistic regression between the selected points and the remaining data.\n",
    "\n",
    "    This version takes a sparse feature matrix and column_index_dictionary which maps from the\n",
    "    indices of the matrix to the set of feature names.\n",
    "\n",
    "    Then it displays that feature importance in a bar plot.\n",
    "    The title is colour coded by model accuracy in order to give a rough approximation of\n",
    "    how much trust you should put in the model.\n",
    "\n",
    "    All of the standard caveats with using the coefficients of a linear model as a feature\n",
    "    importance measure should be included here.\n",
    "\n",
    "    It might be worth reading the sklearn documentation on the\n",
    "    common pitfalls in the interpretation of coefficients of linear models\n",
    "    (https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    data: sparse_matrix\n",
    "        A sparse_matrix corresponding to the plot points.\n",
    "    column_index_dictionary: dict\n",
    "        A dictionary mapping from column indices to column names\n",
    "    max_features: int <default: 15>\n",
    "        The maximum number of features to display the importance for.\n",
    "    tol_importance_relative: float <default: 0.01>\n",
    "        The minimum feature coefficient value in order to be considered important.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        column_index_dictionary,\n",
    "        max_features: int = 15,\n",
    "        tol_importance_relative: float = 0.01,\n",
    "    ):\n",
    "\n",
    "        self.data = data  # Indexed 0 to length.\n",
    "        self.max_features = max_features\n",
    "        self.tol_importance_relative = tol_importance_relative\n",
    "        self._features = column_index_dictionary\n",
    "        self._classifier = None\n",
    "        self._classes = None\n",
    "\n",
    "    def summarize(self, selected, width: int = 600, height: int = 600):\n",
    "        classes = np.zeros((self.data.shape[0],), dtype=\"int32\")\n",
    "        classes[selected] = True\n",
    "        classifier = LogisticRegression(\n",
    "            penalty=\"l1\",\n",
    "            solver=\"liblinear\",\n",
    "            class_weight=\"balanced\",\n",
    "            tol=1e-3,\n",
    "            max_iter=20\n",
    "        ).fit(self.data, classes)\n",
    "        self._classifier = classifier\n",
    "        self._classes = classes\n",
    "        assert classifier.coef_.shape[0] == 1 or classifier.coef_.ndim == 1\n",
    "        importance = np.squeeze(classifier.coef_)\n",
    "        index_importance = np.argsort(-np.abs(importance))[: self.max_features]\n",
    "        importance_abs = np.abs(importance)[index_importance]\n",
    "        importance_relative = importance_abs / np.max(importance_abs)\n",
    "        importance_restricted = importance[\n",
    "            np.where(importance_relative > self.tol_importance_relative)\n",
    "        ]\n",
    "\n",
    "        selected_columns_tuples = [self._features[x] for x in index_importance[: len(importance_restricted)] ]\n",
    "        selected_columns = [f\"{kind}: {value}\" for kind, value in selected_columns_tuples]\n",
    "\n",
    "        model_acc = classifier.score(self.data, classes)\n",
    "        fig = bpl.figure(\n",
    "            y_range=selected_columns,\n",
    "            width=width,\n",
    "            height=height,\n",
    "        )\n",
    "        if model_acc > 0.9:\n",
    "            fig.title = f\"Estimated Feature Importance\\nTrustworthiness high ({model_acc:.4} mean accuracy)\"\n",
    "            fig.title.text_color = \"green\"\n",
    "        elif model_acc > 0.8:\n",
    "            fig.title = f\"Estimated Feature Importance\\nTrustworthiness medium ({model_acc:.4} mean accuracy)\"\n",
    "            fig.title.text_color = \"yellow\"\n",
    "        elif model_acc > 0.5:\n",
    "            fig.title = f\"Estimated Feature Importance\\nTrustworthiness low ({model_acc:.4} mean accuracy)\"\n",
    "            fig.title.text_color = \"orange\"\n",
    "        else:\n",
    "            fig.title = f\"Estimated Feature Importance\\nTrustworthiness low ({model_acc:.4} mean accuracy)\"\n",
    "            fig.title.text_color = \"red\"\n",
    "\n",
    "        fig.hbar(\n",
    "            y=selected_columns,\n",
    "            right=importance[index_importance[: len(importance_restricted)]],\n",
    "            height=0.8,\n",
    "        )\n",
    "        plt.xlabel(\"Coefficient values corrected by the feature's std dev\")\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de249c6f-ee27-48cc-bec9-a6265df95b5b",
   "metadata": {},
   "source": [
    "Expect these two summarizers to be baked into a new version of TNT coming soon to a repository near you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac2819-ad68-4dfe-a01d-30dd246f9d70",
   "metadata": {},
   "source": [
    "## Preparing metadata for prime time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9edd8a-35d5-44cd-980c-1f167fbb0c5e",
   "metadata": {},
   "source": [
    "Let's supplement the metadata dataframe we have for our top-15 processes with data that aids visualization better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c847eb-71ee-418e-b3b3-3f6667764360",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metadata_summary_top15 = pd.merge(\n",
    "    metadata_top15,\n",
    "    CategoricalColumnTransformer(\n",
    "        object_column_name='process_id',\n",
    "        descriptor_column_name=list(metadata_top15.columns[2:]),\n",
    "        include_column_name=True\n",
    "    ).fit_transform(metadata_top15.astype('str')).rename(\"event_summary\").reset_index(),\n",
    "    on=\"process_id\",\n",
    "    how=\"left\"\n",
    ").merge(labels_top15, on=\"process_id\", how=\"left\")\n",
    "metadata_summary_top15[\"event_summary_string\"] = metadata_summary_top15[\"event_summary\"].apply(lambda x: \"<br>\".join(x))\n",
    "metadata_summary_top15[\"freq\"] = 1\n",
    "metadata_summary_top15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f759c8aa-5e42-45a3-a0fc-5f95811d99cc",
   "metadata": {},
   "source": [
    "Let's also create an alternative feature dictionary: features that correspond to filesystem paths tend to be obnoxiously long strings.\n",
    "It is useful to clip these to the file name at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325715cc-e77f-41b5-abd1-fce77ec6fa96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "col2token_viz = {i: (kind, value.split(\"\\\\\")[-1]) for i, (kind, value) in col2token.items()}\n",
    "assert len(col2token_viz) == features.shape[1]\n",
    "for i, (k, v) in enumerate(col2token_viz.items()):\n",
    "    if i >= 25:\n",
    "        break\n",
    "    print(k, \":\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c0c414-7ff5-42e6-a544-7e894b610fe8",
   "metadata": {},
   "source": [
    "## Hierarchical map annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e1599e-6e5c-42fd-8ffc-7b0d394e0530",
   "metadata": {},
   "source": [
    "While a naked map might make sense to somebody who has explored the territory it describes,\n",
    "it tends to be much easier to use when it is *annotated* with the names of locations,\n",
    "roads and other landmarks.\n",
    "The same goes for data maps.\n",
    "TNT provides various tools to automate the production of a hierarchical set of annotations from the features of the dataset:\n",
    "feature values displayed over clusters at various magnification scales.\n",
    "It is tricky to describe but joyful to use.\n",
    "The following code produce such a hierarchy of annotations for the top-15 data.\n",
    "**Be patient**: producing these annotations takes up to 3 minutes on a M1-generation MacBook Pro;\n",
    "it may take several minutes up to half an hour on computers with fewer resources.\n",
    "If one is using the canned data, we skip the calculations altogether and get the object out of the Azure Blob container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920aeecb-db55-4c88-a555-58dc97a2c42c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if FS.isfile(f\"{ROOT}/layers.pkl.gz\"):\n",
    "    with (\n",
    "        FS.open(f\"{ROOT}/layers.pkl.gz\", \"rb\") as file_gzip,\n",
    "        gzip.open(file_gzip, \"rb\") as file_hier\n",
    "    ):\n",
    "        hier_annotations = cpkl.load(file_hier)\n",
    "else:\n",
    "    infoweight = InformationWeightTransformer().fit_transform(features_top15).astype(np.float32)\n",
    "    infoweight_compressed = TruncatedSVD(n_components=1024).fit_transform(infoweight)\n",
    "    layer_metadata = tnt.SparseMetadataLabelLayers(\n",
    "        infoweight_compressed,\n",
    "        processes_top15,\n",
    "        features_top15,\n",
    "        {i: value for i, (_, value) in col2token_viz.items()},\n",
    "        cluster_map_representation=False,\n",
    "        random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f1cf51-905e-47a3-b44b-2c48625855fe",
   "metadata": {},
   "source": [
    "## Bringing it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d23b3f-cab4-4199-909c-2b0f1d94da86",
   "metadata": {},
   "source": [
    "We finally have all the ingredients to put together our interactive dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcfbf3d-eada-4e7f-8d3d-83ff4c09a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scatter plot, hierarchically annotated.\n",
    "plot = tnt.BokehPlotPane(\n",
    "    processes_top15,\n",
    "    labels=labels_top15[\"label\"],\n",
    "    width=600,\n",
    "    height=600,\n",
    "    show_legend=False,\n",
    "    tools=\"pan,wheel_zoom,lasso_select,tap,reset\"\n",
    ")\n",
    "plot.add_cluster_labels(hier_annotations, max_text_size=24)\n",
    "\n",
    "# This widget enables the edition of the categorical labels associated to the various points (one label per point).\n",
    "editor = tnt.LabelEditorWidget(plot.labels, selectable_legend=True)\n",
    "editor.link_to_plot(plot)\n",
    "\n",
    "# This is one of our most simple search widgets.  Please see our read the docs page for more powerful and flexible search options.\n",
    "search = tnt.KeywordSearchWidget(\n",
    "    pd.Series([\n",
    "        \" \".join(col2token[rc[1]][1] for rc in rcs)\n",
    "        for _, rcs in it.groupby(\n",
    "            np.stack(np.nonzero(features_top15)).T,\n",
    "            key=lambda rc: rc[0]\n",
    "        )\n",
    "    ])\n",
    ")\n",
    "search.link_to_plot(plot)\n",
    "\n",
    "# A widget to tweak plot properties in order to study the distribution of various features.\n",
    "control_df = metadata_top15[\"THREAD,FLOW,PROCESS,FILE,REGISTRY,TASK,MODULE,USER_SESSION,SERVICE,SHELL,HOST\".split(',')]\n",
    "control = tnt.PlotControlWidget(raw_dataframe=control_df)\n",
    "control.link_to_plot(plot)\n",
    "\n",
    "# A pane to display detailed information about a single selected process.\n",
    "info_pane = tnt.InformationPane(\n",
    "    metadata_summary_top15,\n",
    "    markdown_template=\"\"\"\\\n",
    "# {label}\n",
    "\n",
    "## {process_id}\n",
    "\n",
    "---\n",
    "\n",
    "{event_summary_string}\n",
    "\"\"\",\n",
    "    width=600)\n",
    "info_pane.link_to_plot(plot)\n",
    "\n",
    "# Class counts among selected data.\n",
    "value_summarizer = tnt.summary.dataframe.ValueCountsSummarizer(labels_top15[\"label\"])\n",
    "value_summary_plot = tnt.summary.dataframe.DataSummaryPane(value_summarizer)\n",
    "value_summary_plot.link_to_plot(plot)\n",
    "\n",
    "# Time series summary of the occurrence of selected processes.\n",
    "time_summarizer = tnt.summary.plot.TimeSeriesSummarizer(\n",
    "    metadata_summary_top15,\n",
    "    time_column='timestamp',\n",
    "    count_column='freq'\n",
    ")\n",
    "time_summary_plot = tnt.summary.plot.PlotSummaryPane(time_summarizer)\n",
    "time_summary_plot.link_to_plot(plot)\n",
    "\n",
    "# Characterization of selected data by common feature support.\n",
    "support_summarizer = SparseSupportSummarizer(features_top15, col2token_viz, max_features=16)\n",
    "support_summary_df = tnt.summary.dataframe.DataSummaryPane(support_summarizer, width=600, sizing_mode=None)\n",
    "support_summary_df.link_to_plot(plot)\n",
    "\n",
    "# Feature importance summary for selected data.\n",
    "feature_summarizer = SparseFeatureImportanceSummarizer(features_top15, col2token_viz, max_features=8)\n",
    "feature_summary_plot = tnt.summary.plot.PlotSummaryPane(feature_summarizer, width=800, sizing_mode=\"stretch_both\")\n",
    "feature_summary_plot.link_to_plot(plot)\n",
    "\n",
    "#Lay out the widgets that you are interested in using via Panel's excellent Row, Column and Tab functions.\n",
    "pn.Column(\n",
    "    pn.Row(plot, pn.Column(pn.Row(editor, pn.Column(search, control)))),\n",
    "    pn.Tabs(\n",
    "        (\"Chronology\", pn.Row(time_summary_plot, value_summary_plot)),\n",
    "        (\"Feature importance\", pn.Row(feature_summary_plot, support_summary_df)),\n",
    "        (\"Details\", info_pane))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e56a972-09c5-4c83-9d16-4ac81e021320",
   "metadata": {},
   "source": [
    "Fun and/or useful things to do with this dashboard:\n",
    "\n",
    "1. [ ] Pan and zoom around the scatter plot, watching feature tokens that characterize clusters come in and out of focus.\n",
    "1. [ ] Select all processes involving a feature with the word `ping`\n",
    "    - Click on the **Search** button: typing Enter at the keyboard does not always work. \n",
    "1. [ ] Select a group of points with the lasso.\n",
    "1. [ ] Count the number of processes from each class in a selected clump.\n",
    "1. [ ] Appreciate when a clump of processes occured through the dataset's timeline.\n",
    "1. [ ] Look up how various features support the similarity between selected processes.\n",
    "1. [ ] Look up the features of most importance with respect to differentiating some selected points from others.\n",
    "    - **Remark**: the model training computed on the fly sometimes takes a few seconds to complete, so be patient when this plot fails to update rapidly.\n",
    "1. [ ] Look up the number of each event types that were generated in a single selected process instance.\n",
    "1. [ ] Grab the indices of selected process through `plot.selected`.\n",
    "1. [ ] Appreciate, using marker sizes, the processes having generated by the most events targeting `FLOW` objects; contrast with those having generated the most events targeting `FILE` objects.\n",
    "1. [ ] Change the process class `tasklist.exe` to `tasklist` to merge the former to the latter.\n",
    "    - The old color difference remains; it's a known bug.\n",
    "1. [ ] Select a cluster of points and create a new label for them; name them according to a feature or two that separate them cleanly from others.\n",
    "1. [ ] Grab the labels generated from mergings and new label creations through `plot.labels`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Commonplace detection in categorical telemetry data",
   "language": "python",
   "name": "commonplace-detection-optc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
