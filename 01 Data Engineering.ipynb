{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1923bd9-19fd-488a-bace-9fabe5526294",
   "metadata": {},
   "source": [
    "**Commonplace detection in categorical telemetry data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dac75b-eb84-44f0-a2cf-1680b862040d",
   "metadata": {},
   "source": [
    "# Data engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deedec36-3d3b-426c-8f1b-f1f2996481b9",
   "metadata": {},
   "source": [
    "This notebook exemplifies how to encode telemetry events as a sequence of *bags of categorical labels*, and create a vector representation of this data that is suitable to visualization.\n",
    "\n",
    "**Warning**: the implementation of this data engineering pipeline is memory- and CPU-intensive.\n",
    "It strictly requires at least 8 GB of RAM and 2 CPU threads to run to success (the configuration has not even been tested, any confirmation of success with such a configuration would be appreciated);\n",
    "16 GB with 8 CPU threads is recommended.\n",
    "Larger resource budgets will be leveraged usefully, up to about 64 CPUs and 256 GB of RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd9cc5-6642-4b53-b69d-b94651fb837d",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc498ece-d5d3-4129-9f5e-59d6636c87aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cloudpickle as cpkl\n",
    "from collections import defaultdict\n",
    "from dask import delayed\n",
    "from dask.distributed import LocalCluster, Client, as_completed\n",
    "import fsspec\n",
    "import gzip\n",
    "from hashlib import md5\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as ss\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import struct\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "import umap\n",
    "import vectorizers as vz\n",
    "import zstandard as zstd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a293c945-58a1-4d8e-9b47-7a17bb0f85f5",
   "metadata": {},
   "source": [
    "This work focuses on the [Operationally Transparent Cyber Data Release](https://github.com/FiveDirections/OpTC-data), abbreviated to the **OpTC dataset**.\n",
    "The stored form of this dataset minimizes storage cost rather than facilitate processing.\n",
    "Thus, I publish here a prefiltering of the host-based sensor telemetry\n",
    "(encoded in GZIP-compressed JSON monoliths according to the [extended CAR](https://github.com/FiveDirections/OpTC-data/blob/master/ecar.md) data model)\n",
    "into smaller, more numerous chunks of ZSTD-compressed JSON for 3 of the more interesting hosts\n",
    "(out of the 625 hosts that compose the whole dataset).\n",
    "This data can be accessed through the following Azure Blob,\n",
    "whose hosting is a courtesy of the Tutte Institute for Mathematics and Computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88831105-06b7-4c8c-ac9a-89d591d4479b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\"abfs\", account_name=\"scipy2023\")\n",
    "fs.ls(\"optc/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57fd4c-c497-43b3-9926-95ee0241d349",
   "metadata": {},
   "source": [
    "Let's use the local machine as a ad hoc Dask cluster.\n",
    "If you can access a better cluster, change the next cell to instantiate the Dask `Client` against it.\n",
    "Remark that it is critical for the memory limit for each worker to be at least 6 GB.\n",
    "Only one Dask worker will reach up close this limit,\n",
    "so in principle this should not freeze the OS even if we allow the set of workers to manage more than the total amount of RAM available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b4c53-37dd-495f-82ed-92bab04c5efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(threads_per_worker=1, memory_limit=\"6GB\")\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0545cacf-49be-44b8-822c-1d5a927cb954",
   "metadata": {},
   "source": [
    "We will work on host 501, processing all days.\n",
    "One may also choose host 201 or 351 if curious.\n",
    "The timespan of the following analysis covers the whole data capture span, from September 17 to 25 (2019).\n",
    "In a compute pinch, one may select a subset of these days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dceeb8d-d457-434c-a278-a44b13291cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = 501\n",
    "HOSTNAME = f\"SysClient{HOST:04d}.systemia.com\"\n",
    "DAYS = [\"*\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0a2459-8318-46de-bf5a-70751dcb09fc",
   "metadata": {},
   "source": [
    "Here are the *chunk* files: the data engineered so it can be consumed more easily using our Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ea90e-5eb6-4cb6-8d3f-502f943460a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_HOSTNAME = f\"optc/{HOSTNAME}\"\n",
    "CHUNKS = sorted(sum(\n",
    "    [fs.glob(f\"optc/{HOSTNAME}/{day}/chunk.*.json.zstd\") for day in DAYS],\n",
    "    []\n",
    "))\n",
    "len(CHUNKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f47c48-65d6-4bac-b46e-59b5a1e8a086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHUNKS[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161938e-281f-4715-bcbb-b6d37509e62f",
   "metadata": {},
   "source": [
    "The JSON files of host-based telemetry are composed of events whose exact set of fields is determined by the type of **object** changed by the event\n",
    "(and, to a lesser degree, by the **action** that changes it).\n",
    "The following filters the field for each object type to retain only those that can be usefully understood as characterized by *categorical* values without any modification\n",
    "(such as number binning or token lemmatization based on cyber expertise).\n",
    "\n",
    "This simplest of approaches lends itself to much faster encoding of the categorical values:\n",
    "indeed, value transformations often involve string operations to such a volume as to incur at least twice the computational cost of the following feature extraction code.\n",
    "However, the cost of this more expensive work can be tamed and, most importantly, amortized by saving the non-explicit values (or their intermediate representation).\n",
    "Having experimented with various degrees of sophistication to this task, I believe that what follows gives a pretty good gist of the correlations between events and their aggregates:\n",
    "more sophisticated tokenization approaches appear to provide value, but the difference is not so remarkable as to warrant obfuscating this presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f356e4a-325c-45ac-b7d4-34cba7c9b186",
   "metadata": {},
   "source": [
    "The following data structure is slightly hard to understand,\n",
    "as it provides layers of simplification to avoid tedious boilerplate.\n",
    "In principle, it consists in a hierarchical dictionary whose leaves are sequences of categorical field descriptors.\n",
    "The full hierarchy follows the convention:\n",
    "\n",
    "```\n",
    "object type\n",
    "    action type\n",
    "        computation identifier name\n",
    "            sequence of categorical field descriptors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea8a08a-6f9e-420a-be76-a67af0b4de08",
   "metadata": {},
   "source": [
    "Starting from the bottom, a *categorical field descriptor* consists in the name of the field in the JSON object that describes the event, as well as its _kind_ -- the nature of the object, which makes it more comparable to others.\n",
    "For instance, two fields named respectively `src_ip` and `dest_ip` would both have as value IP addresses: they are thus of the same kind, and a token of nominal value `10.100.23.34` yield a correlation between two events even if they carry it over these two fields respectively.\n",
    "However, if a hypothetical process were loaded from a file named `10.100.23.34`, and we did not set its kind as a `path`, it might be confused with an IP address token with the same value, yielding an undue correlation.\n",
    "Such is thus the purpose of kinds.\n",
    "Finally, many fields carry token whose kind is eponymous, so no need to have a repetitive tuple -- these are described with just the field name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d7540-7762-45cb-9cff-9b1362bb1240",
   "metadata": {},
   "source": [
    "We assume that each event happened as part of a single *computation*: a bit of code running for a certain purpose.\n",
    "As we will describe <a id=\"event_grouping\"></a>[below](#events_grouped_by_processes), we construe such computations as *process instances*.\n",
    "For most events, we can thus identify each computation by the process that was responsible for the occurrence of the event.\n",
    "This process is uniquely identified by a UUID generated during data capture, and stored as field `actorID`.\n",
    "There is, however, one special case: when a process is created (object type `PROCESS`, action `CREATE`),\n",
    "we don't merely consider this as an action taken by the parent process.\n",
    "We also make this event the first that occurs in the new process:\n",
    "this induces useful correlations between processes that spawn the same children,\n",
    "as well as between processes born from similar parents.\n",
    "Thus, we represent this as if two events had occurred:\n",
    "the creation of the child process,\n",
    "under the responsibility of its parent;\n",
    "and the start of the new process,\n",
    "under its own responsibility.\n",
    "In the latter case, the UUID for the child object is stored as the `objectID` field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c237de-888d-41e9-a9a7-3121bd03f249",
   "metadata": {},
   "source": [
    "Finally, for most object types, we harvest values from the same subset of fields, regardless of action type.\n",
    "The exception of process creations is present here again,\n",
    "because the characterization of the spawning of a child process\n",
    "and that of the start of a new process in itself are different,\n",
    "and thus leverage distinct fields.\n",
    "These are irrelevant to the other actions targeting a `PROCESS` object.\n",
    "Thus, in most cases, the hierarchy is elided to\n",
    "\n",
    "```\n",
    "object type\n",
    "    sequence of field descriptors (with eponymous kinds omitted)\n",
    "```\n",
    "\n",
    "The full hierarchy is only declined with `PROCESS` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceef86f1-43c8-4eef-a3b6-5a7161a7ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals_easy = {\n",
    "    \"FLOW\": [\"object\", \"action\", (\"src_ip\", \"ip\"), (\"dest_ip\", \"ip\"), (\"src_port\", \"port\"), (\"dest_port\", \"port\"), \"l4protocol\", \"direction\"],\n",
    "    \"FILE\": [\"object\", \"action\", (\"file_path\", \"path\"), \"info_class\", (\"new_path\", \"path\")],\n",
    "    \"HOST\": [\"object\", \"action\"],\n",
    "    \"MODULE\": [\"object\", \"action\", (\"module_path\", \"path\")],\n",
    "    \"REGISTRY\": [\"object\", \"action\", (\"key\", \"registry-key\"), (\"value\", \"registry-value\"), (\"type\", \"registry-type\")],\n",
    "    \"SERVICE\": [\"object\", \"action\", (\"name\", \"service-name\")],\n",
    "    \"SHELL\": [\"object\", \"action\"],\n",
    "    \"TASK\": [\"object\", \"action\", \"path\", (\"task_name\", \"task-name\")],\n",
    "    \"THREAD\": [\"object\", \"action\"],\n",
    "    \"USER_SESSION\": [\"object\", \"action\", (\"user\", \"user-domain\"), (\"requesting_domain\", \"domain\"), (\"requesting_user\", \"user\"), (\"src_ip\", \"ip\"), (\"src_port\", \"port\")],\n",
    "    \"PROCESS\": {\n",
    "        \"CREATE\": {\n",
    "            \"actorID\": [\"object\", \"action\", (\"image_path\", \"child\"), (\"image_path\", \"path\")],\n",
    "            \"objectID\": [(\"parent_image_path\", \"parent\"), (\"image_path\", \"process\"), (\"user\", \"user-domain\")]\n",
    "        },\n",
    "        \"OPEN\": {\n",
    "            \"actorID\": [\"object\", \"action\"]\n",
    "        },\n",
    "        \"TERMINATE\": {\n",
    "            \"actorID\": [\"object\", \"action\"]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12047b2c-b209-4014-9122-f6b6bfbad2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_events(path_chunk):\n",
    "    \"\"\"\n",
    "    Generates the sequence of events (as JSON objects, hence dictionaries) decoded out of a *chunk* --\n",
    "    a ZSTD-compressed JSON Lines file.\n",
    "    \"\"\"\n",
    "    with fs.open(path_chunk, \"rb\") as file_raw, zstd.open(file_raw, mode=\"rt\", encoding=\"utf-8\") as file_decompressed:\n",
    "        for line in file_decompressed:\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip ill-formed records.\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d01542-e066-4d55-907a-918756a2cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(path_chunk):\n",
    "    \"\"\"\n",
    "    Filters the events generated out of a *data chunk* so that each is reported as a\n",
    "    3-tuple composed of timestamp, computation identifier and its characterization as a\n",
    "    sequence of categorical tokens.\n",
    "    \"\"\"\n",
    "    for event in iter_events(path_chunk):\n",
    "        obj = event[\"object\"]\n",
    "        if obj not in categoricals_easy:\n",
    "            continue\n",
    "        categoricals = (\n",
    "            categoricals_easy[obj].get(event[\"action\"], {})\n",
    "            if isinstance(categoricals_easy[obj], dict)\n",
    "            else {\"actorID\": categoricals_easy[obj]}\n",
    "        )\n",
    "        for identifier, features in categoricals.items():\n",
    "            tokens = []\n",
    "            for feature in features:\n",
    "                field, kind = (feature, feature) if isinstance(feature, str) else feature\n",
    "                if value := event.get(field, \"\"):\n",
    "                    tokens.append((kind, value))\n",
    "            yield (pd.Timestamp(event[\"timestamp\"]), event[identifier], tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c5fda-f251-411d-9c39-ef7abcb2fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabulate_features(path_chunk):\n",
    "    \"\"\"\n",
    "    Encodes the sequence of events and their extracted features into a Pandas dataframe.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(\n",
    "        data=extract_features(path_chunk),\n",
    "        columns=[\"timestamp\", \"process_id\", \"tokens\"]\n",
    "    ).astype({\"process_id\": \"category\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df45a24-a585-4f79-a09c-6691f64d524e",
   "metadata": {},
   "source": [
    "The feature engineering plan to enable visualization on the basis of *similarity of computations* is as follows:\n",
    "\n",
    "1. Encode each event into a vector using *one-hot encoding*.\n",
    "1. Drop process vectors that are not described by enough events for their representation to mean much.\n",
    "1. Group events by computation identifier, producing a vector representation for each computation (process) as the **sum** of its events.\n",
    "1. Clean up the feature space, getting rid of non-informative features such as [spurious values](https://medium.com/@saitejaponugoti/stop-words-in-nlp-5b248dadad47) and *orphan values* (which describe too few events).\n",
    "1. Compress the vector representation by [manifold learning](https://scikit-learn.org/stable/modules/manifold.html) down to two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a624857-94df-4ea8-889b-de053af2ead1",
   "metadata": {},
   "source": [
    "## Step 1: computing the event feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53458360-dc87-4042-93af-8c13b0c97639",
   "metadata": {},
   "source": [
    "[One-hot encoding](https://en.wikipedia.org/wiki/One-hot) is a process of for computing [multisets](https://en.wikipedia.org/wiki/Multiset) of categorical *tokens* into vectors of value counts.\n",
    "One associates each multiset to one row of the matrix, and each token of the sum-union of all multisets (the *vocabulary* of the dataset) to a column of the matrix.\n",
    "Each matrix entry $(i, j)$ then corresponds to the number of occurrences of token $j$ in multiset $i$.\n",
    "Such numbers of occurrences are often referred to as token *weights*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def7365-8090-445f-8d58-715fc36720ea",
   "metadata": {},
   "source": [
    "At the Tutte Institute, we have put together the [Vectorizers](https://vectorizers.readthedocs.io/en/latest/) library as a repository of our tricks for turning variable-length data objects into fixed-length vectors.\n",
    "In particular, class `NgramVectorizer`, instantiated to handle a vocabulary of 1-grams (e.g. normal tokens), not only performs one-hot encoding,\n",
    "but also provides,\n",
    "through its `+` operator,\n",
    "a way to merge the encodings of a segmented dataset.\n",
    "It is thus ideal for the handling of our set chunk files of telemetry.\n",
    "Once each chunk is encoded, we sum them hierarchically.\n",
    "(I wanted to do that with a [Dask bag](https://docs.dask.org/en/stable/bag.html),\n",
    "but some bug frustrates me, so I got on with my own sum implementation.)\n",
    "\n",
    "This operation is memory-intensive, as *a whole lot* of data is summarized into the sparse `event_matrix`.\n",
    "Please tolerate Dask's nervous chatter regarding memory usage and garbage collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08744dbc-7da1-4a70-8a22-a5cd8f93fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_features(path_chunk):\n",
    "    return vz.NgramVectorizer().fit(tabulate_features(path_chunk)[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0d9a9-fe84-433d-b683-9b4af72cae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "summands = [[delayed(vectorize_features)(chunk) for chunk in CHUNKS]]\n",
    "while len(summands[-1]) > 1:\n",
    "    to_sum = summands[-1]\n",
    "    sums = []\n",
    "    for i in range(0, len(to_sum), 2):\n",
    "        if i + 1 < len(to_sum):\n",
    "            sums.append(to_sum[i] + to_sum[i + 1])\n",
    "        else:\n",
    "            sums.append(to_sum[i])\n",
    "    summands.append(sums)\n",
    "\n",
    "futs = client.compute(sum(summands, []))\n",
    "for fut in tqdm(as_completed(futs), total=sum(len(ss) for ss in summands)):\n",
    "    pass\n",
    "\n",
    "vzr_all = futs[-1].result()\n",
    "event_matrix = vzr_all._train_matrix\n",
    "client.cancel(futs)\n",
    "del futs\n",
    "event_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffaa441-0940-4d67-9c57-f4da8e1cf82d",
   "metadata": {},
   "source": [
    "## Steps 2 and 3: grouping events into processes, and eliminating ill-represented processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2117a92-e646-4e58-8f52-d9609578fdcb",
   "metadata": {},
   "source": [
    "<a id=\"events_grouped_by_processes\"></a>\n",
    "We have discussed [earlier](#event_grouping) that event subsequences form coherent *computations*.\n",
    "While there are many ways to cut subsequences in order to delineate so as to delineate their purpose,\n",
    "the approach immediately at hand is to focus on *process instances*.\n",
    "Modern general-purpose operating systems, when asked to run a certain program, box this computation into a *process*:\n",
    "it serves as a unit to allocate resources and protect memory structure from other processes.\n",
    "The OpTC dataset does a great job of associating events to the process instances that generated them,\n",
    "and most processes have good enough unity of purpose for us to latch onto them as the context around which to group events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a604907-e212-41c1-9345-870cbb66763b",
   "metadata": {},
   "source": [
    "One very simple but hacky way to combine the vector representations of events into one for each process is to **sum** (or **average**) these vectors together.\n",
    "This effectively characterizes each process as the number of times any categorical value occured through the events that it generated.\n",
    "This representation discards a lot of information: event timing, event order, and even the boundaries between events.\n",
    "However, it will make the vector distance between two processes where the same events be low, hence highly similar.\n",
    "It induces undue similarity in case of different event timing, order, or even nature, but we go in knowing that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc87d584-d837-4c82-9101-9020dccd35c2",
   "metadata": {},
   "source": [
    "An issue that arises here is that the number of events generated by each process varies a lot.\n",
    "It makes little sense to analyze the regularity of the behavior of processes that generate very few events.\n",
    "Let's thus first analyze the distribution of total process weights as a proxy to the notion of *proper* process characterization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1686e2b9-0181-4623-b6ec-67e27c50c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_processes(metadata):\n",
    "    return metadata.groupby(\"process_id\", as_index=False).agg({\"timestamp\": \"min\", **{col: \"sum\" for col in metadata.columns if col not in {\"timestamp\", \"process_id\"}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6232e20-8c4c-4a4b-bf13-fc4589260059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_by_process(path_chunk):\n",
    "    features = tabulate_features(path_chunk)\n",
    "    metadata = features[[\"timestamp\", \"process_id\"]].join(\n",
    "        pd.DataFrame(\n",
    "            data=iter(features[\"tokens\"].apply(lambda tokens: {value: 1.0 for kind, value in tokens if kind == \"object\"}))\n",
    "        ),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    metadata[\"event_index\"] = pd.Series(metadata.index).apply(lambda x: [x])\n",
    "    return summarize_processes(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be92c75-fd6c-4a61-a946-c4463173d21c",
   "metadata": {},
   "source": [
    "To stay abreast of what's going on, let's eyeball an example of the summary of a process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883b944-b1d2-4807-bbd5-e5624ced5889",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "events_by_process(CHUNKS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37384278-125a-4620-8f5e-752d7625e42a",
   "metadata": {},
   "source": [
    "The timestamp of the process corresponds to that of the first event found associated to it over the chunk's sequence.\n",
    "We also count the number of each type of events, denoted by `object` type.\n",
    "Finally, we keep a list of the event indices associated to each process.\n",
    "Let's merge these latter index lists over all chunks,\n",
    "building a dictionary that maps process identifiers to lists of event index not in each chunk respectively,\n",
    "but rather in the `event_matrix` built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf827d1-7465-4665-907d-fa4e3f73d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "process2ievent = {}\n",
    "total_events = 0\n",
    "metadata_processes = pd.DataFrame()\n",
    "for fut in tqdm(client.map(events_by_process, CHUNKS), total=len(CHUNKS)):\n",
    "    processes = fut.result()\n",
    "    metadata_processes = summarize_processes(pd.concat([metadata_processes, processes.drop(columns=[\"event_index\"])], ignore_index=True).fillna(0.0))\n",
    "    for process_id, indices in processes[[\"process_id\", \"event_index\"]].itertuples(index=False):\n",
    "        process2ievent.setdefault(process_id, [])\n",
    "        for index_row_chunk in indices:\n",
    "            process2ievent[process_id].append(index_row_chunk + total_events)\n",
    "    total_events += processes[\"event_index\"].apply(len).sum()\n",
    "\n",
    "len(process2ievent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa53042-383d-4254-b7bb-3ed345f51102",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29fbf1f-e5ff-405b-8b66-75459196f30d",
   "metadata": {},
   "source": [
    "That's *very many* processes.\n",
    "To determine which of these don't carry enough features to be reliably characterized compared to others,\n",
    "let's look at the empirical distribution of weight totals (over logarithmic bins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994424d-06f7-4fde-be31-3c3d5dea23a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_per_event = np.array(event_matrix.sum(axis=1)).squeeze()\n",
    "features_per_process = pd.Series({process_id: sum([features_per_event[i] for i in indices]) for process_id, indices in tqdm(process2ievent.items())})\n",
    "features_per_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28daf9ef-1cb9-4ee8-b237-b15f1cd9a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_per_process.apply(np.log10).hist(bins=range(-1, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf99a6d-5569-4734-bca7-2c1db71ca0da",
   "metadata": {},
   "source": [
    "It does not make much sense to me to keep processes described by a total number of categorical features less than 10.\n",
    "Let's drop the process instances that make up the first column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5bd62a-70f3-4b84-b52a-864a4afeb58a",
   "metadata": {},
   "source": [
    "<a id=\"pruning\"></a>\n",
    "Since we compute process vectors as sums of event vectors,\n",
    "each process corresponds to a linear combination of the rows of `event_matrix` associated to it\n",
    "(which we stored in `process2ievent`).\n",
    "Let's thus build a *projection matrix* that will be multiplied on the left of `event_matrix` to compute the process vectors.\n",
    "We will restrict the rows of this projection matrix so as to drop the process instances with insufficient features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69941624-7409-4705-8008-a9d93c0d7569",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "irows = []\n",
    "icols = []\n",
    "process2irow = {}\n",
    "irow2process = {}\n",
    "irow_next = 0\n",
    "for process_id, indices in tqdm(process2ievent.items()):\n",
    "    if features_per_process.loc[process_id] >= 10:\n",
    "        irow = irow_next\n",
    "        irow_next += 1\n",
    "        irows += [irow] * len(indices)\n",
    "        icols += indices\n",
    "        process2irow[process_id] = irow\n",
    "        irow2process[irow] = process_id\n",
    "\n",
    "projection = ss.coo_matrix((np.ones((len(irows),), dtype=np.int32), (irows, icols)), shape=(len(process2irow), event_matrix.shape[0])).tocsr()\n",
    "assert set(np.array(projection.sum(axis=0)).squeeze()) <= {0, 1}\n",
    "projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbaf50a-b443-423f-bbae-21b8fcb2fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_matrix = (projection @ event_matrix).astype(np.float32)\n",
    "process_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e398b-56f2-4620-a0f5-6a42d01f90c8",
   "metadata": {},
   "source": [
    "Let's prune the process metadata frame to mirror the rows of `process_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91de69-93bd-4f95-ae6e-391c374f1865",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned = sorted(list(irow2process.items()))\n",
    "metadata_pruned = metadata_processes.set_index(\"process_id\").loc[[process_id for _, process_id in pruned]].copy().reset_index()\n",
    "assert pd.Series(metadata_pruned.index).equals(pd.Series([i for i, _ in pruned]))\n",
    "metadata_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aed01a-a10e-485c-8747-d96010c52ff7",
   "metadata": {},
   "source": [
    "# Step 4: feature space clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae6cec9-d862-49eb-a042-1a1860ecf0ae",
   "metadata": {},
   "source": [
    "The vector representation we create means to visualize the *similarity structure* present between process instances.\n",
    "As such, we have to focus on features that embody this structure.\n",
    "There are two categories of features that fail to do this, and are thus best discarded; they are uninformative.\n",
    "On the one hand, *spurious features* are shared at roughly the same weight by too many processes, so they hamper the differentiation between processes.\n",
    "On the other hand, *orphan features* occur only to a very small group of processes, so they distort the actual similitude between processes.\n",
    "A look at the distribution of the total weight associated to each feature helps determine how to cull the uninformative.\n",
    "There is also a special class of the latter:\n",
    "features that occur to no process at all,\n",
    "as the [pruning](#pruning) we just did discarded all the process instances that it characterized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc5367-7470-4926-aa62-cb06023bb8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.Series(np.array(process_matrix.sum(axis=0)).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d387dfeb-c56d-4637-9afd-2ed8df8bc359",
   "metadata": {},
   "source": [
    "How many features no longer characterize any process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d5272-4275-4a83-bec6-6ef7dd3ba866",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(feature_importance == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d369f-bc1c-4544-9a6b-912961112c96",
   "metadata": {},
   "source": [
    "Let's histograph the remaining features over logarithmic bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe222baf-776c-4675-a497-f3f1b4ed5164",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.loc[feature_importance > 0].apply(np.log10).hist(bins=[-1,0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c3dd8e-8a08-4d81-a3e8-d4f685e43ef4",
   "metadata": {},
   "source": [
    "Most features, by a large factor, are orphans.\n",
    "We seem not to have any spurious feature, as none is associated to more than 10000 process instances.\n",
    "Let's take a more detailed look at the first column of the previous histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4888b66-79d9-4b6e-911d-339557307037",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.loc[feature_importance < 10].hist(bins=np.linspace(0, 10, 10) - 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16626356-ee67-487f-9ab1-88656a76810a",
   "metadata": {},
   "source": [
    "Again, most of these rarely used features are literal orphans: associated to one or two processes.\n",
    "Let's cull any feature that's not tied to at least 3 processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95468616-527f-40e8-8cf3-0afbbf916904",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "col2token = []\n",
    "token2col = {}\n",
    "indices_keep = []\n",
    "for i, count in enumerate(feature_importance):\n",
    "    if count > 3:\n",
    "        indices_keep.append(i)\n",
    "        token = vzr_all.column_index_dictionary_[i]\n",
    "        index_new = len(col2token)\n",
    "        col2token.append(token)\n",
    "        token2col[token] = index_new\n",
    "\n",
    "culled_matrix = process_matrix[:, indices_keep].copy()\n",
    "culled_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b4bda-1042-483a-b8ff-cbf3e6257aad",
   "metadata": {},
   "source": [
    "The same way that pruning off processes may reduce the usage of a feature to nil,\n",
    "the culling of features may in turn discard the proper representation of processes!\n",
    "Let's check that each process is characterized by a total feature weight of at least 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f04a6-3794-4f97-8aa1-5ec28ac7fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_per_process_redux = np.array(culled_matrix.sum(axis=1)).squeeze()\n",
    "assert np.min(features_per_process_redux) > 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82195045-4033-41bf-a5cb-98d584502846",
   "metadata": {},
   "source": [
    "## Step 5: dimension reduction by manifold learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70607ba-c00d-4363-8a5d-8013230900a8",
   "metadata": {},
   "source": [
    "While t-SNE is a good default choice for compressing vectors to a two-dimension representation for visualization,\n",
    "I rely instead on [UMAP](https://umap-learn.readthedocs.io/en/latest/index.html), from my friends at the Tutte Institute.\n",
    "Its implementation is highly efficient and scalable,\n",
    "and it handles the computation of the $k$ nearest neighbors graph over a very large set of metrics and pseudo-metrics.\n",
    "These include the [Hellinger distance](https://en.wikipedia.org/wiki/Hellinger_distance),\n",
    "the metric best supported by probability theory to assess the similarity between multinomial distributions\n",
    "(which count vectors *are* once $l_1$-normalized).\n",
    "\n",
    "Now, it's easier in practice to compute manifold learning computations on matrices of unique rows.\n",
    "We thus first deduplicate the rows of the culled matrix,\n",
    "keeping an inverse (reduplicating) index so we can still gain a 2D vector representation for all processes under our nose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db52e0b-a075-4e65-b89f-236f957e5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def md5_list(it):\n",
    "    return struct.unpack(\"<QQ\", md5(memoryview(np.array(it))).digest())\n",
    "\n",
    "culled_lil = culled_matrix.tolil()\n",
    "hh = np.zeros(shape=(culled_matrix.shape[0], 4), dtype=np.uint64)\n",
    "for i, indices_values in enumerate(zip(culled_lil.rows, culled_lil.data)):\n",
    "    hh[i, :] = sum((md5_list(it) for it in indices_values), ())\n",
    "_, index_u, inverse_u, counts_u = np.unique(hh, axis=0, return_index=True, return_inverse=True, return_counts=True)\n",
    "index_u.shape, inverse_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5e741-dfdd-413f-8dde-1cb6ebeee2f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_matrix = culled_matrix[index_u, :]\n",
    "unique_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a408b0a-de3f-4910-a523-135b59126cb2",
   "metadata": {},
   "source": [
    "Now, perform the $l_1$ normalization to leverage the Hellinger distance between our process representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a8cfb2-9d23-47d5-b0dc-931ddc7daece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalized_matrix = Normalizer(norm=\"l1\").fit_transform(unique_matrix)\n",
    "normalized_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06b7af-cc96-40ed-8f6d-0d55bb7f929f",
   "metadata": {},
   "source": [
    "We run a variant of UMAP called [densMAP](https://umap-learn.readthedocs.io/en/latest/densmap_demo.html),\n",
    "which strikes a trade-off between agglomeration of similar data vectors\n",
    "and the preservation of density differences between distinct vector neighborhoods in high-dimensional space.\n",
    "This is critical for the detection of the [large dense clusters](https://drive.google.com/file/d/1ZijF656jj7x8AobIypdyQErn0hXOpIFU/view?usp=sharing)\n",
    "of 2D vectors that characterize *commonplace behaviors*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d916a37-6f68-43e7-b9c2-69f3e5d3ab1a",
   "metadata": {},
   "source": [
    "One thing to remark regarding UMAP is that its outcome is not reproducible, even when setting its random seed.\n",
    "Indeed, it runs stochastic gradient descent over distributed computing tools that preclude full randomization anchoring.\n",
    "Therefore, expect that your UMAP computations will be different from mine, but still reflect the same local structures between objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52df97-21d7-43a4-8813-345b50fdd6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "process_protomap = umap.UMAP(\n",
    "    n_components=2,\n",
    "    metric=\"hellinger\",\n",
    "    densmap=True,\n",
    "    dens_lambda=4,\n",
    "    n_epochs=800,\n",
    "    verbose=True\n",
    ").fit_transform(normalized_matrix)\n",
    "process_protomap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb6e48e-76c8-4f9c-83ac-4035d53bd07e",
   "metadata": {},
   "source": [
    "We reduplicate this protomap using the unique inversion index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f9479-4b5a-4950-8129-b2b7a4a391bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_map = process_protomap[inverse_u, :]\n",
    "process_map.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffacf24c-f61a-4a6f-8261-63e543ad8672",
   "metadata": {},
   "source": [
    "## One last thing: process labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5400ed6-18c9-4cd8-a1d5-2e784db059ec",
   "metadata": {},
   "source": [
    "While it is possible to look at a map of processes without any other information,\n",
    "it is much easier to navigate this map if we can rely on certain categorizations of processes, by which we may color them.\n",
    "A natural, if very fine-grain, categorization of processes comes from their **origin**:\n",
    "the artifact that started the software that they contain.\n",
    "The most precise such artifact consists in the **command line** of a process.\n",
    "If the process was started through a terminal or script,\n",
    "the command line carries a lot of information regarding the intent of the user or script author,\n",
    "which is highly useful as far as categories go.\n",
    "The unfortunate aspect of using command line as categorical labels is that in most settings,\n",
    "they are both awfully variable\n",
    "(the cardinal of the vocabulary formed by all command lines making up a dataset can be on the same order of magnitude as the number of processes) and their semantic similarities get lost.\n",
    "An example of this second issue is that the following two invocations of `ping` perform the exact same purpose:\n",
    "\n",
    "```sh\n",
    "ping -w 1000 localhost\n",
    "ping -w 1000 127.0.0.1\n",
    "```\n",
    "\n",
    "So process instances associated to either of these two command lines would be considered to belong to distinct categories,\n",
    "while they actually make up a single one.\n",
    "Another example comes from the usage of full paths to programs, as opposed to lexical elision enabled by such mechanisms as the `PATH` environment variable.\n",
    "Indeed, the following three command lines are semantically the same:\n",
    "\n",
    "```sh\n",
    "tasklist\n",
    "tasklist.exe\n",
    "C:\\Windows\\System32\\tasklist.exe\n",
    "```\n",
    "\n",
    "A full modeling of the categorical space spanned by command lines is outside the scope of this project.\n",
    "Besides, the cardinal of the set of command lines in the OpTC dataset is peculiarly small,\n",
    "reflecting the fact that most of the activity captured in this dataset was performed by automated scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c11b01-134e-4781-88f2-7a22ef3d5047",
   "metadata": {},
   "source": [
    "Unfortunately, not all processes in the dataset map to a PROCESS-CREATE record to document their associated command line.\n",
    "While all processes must effectively start by generating such an event,\n",
    "there is always data loss to host-based sensor systems,\n",
    "no thanks to icky engineering difficulties.\n",
    "So when no command line can be found for a process instance,\n",
    "we fallback on its `image_path`,\n",
    "the path to the executable file whose execution started the process\n",
    "(and which is present as a field to every associated event)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebe057-f3e3-495f-b7dd-881e7af1700c",
   "metadata": {},
   "source": [
    "The capture of a unique label for each process is performed by crunching through all events, enumerating *label proposals* for each process. If such a proposal is a command line, it will take priority over any other.\n",
    "Eventually, we are able to keep none but the top-priority proposal for each process instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41f7dc3-8462-41ca-844b-3b2f74f9e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_labels(proposals):\n",
    "    return proposals.sort_values(\"importance\", ascending=True).drop_duplicates(subset=[\"process_id\"], keep=\"first\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f4fdb-e56b-44e5-97ec-034cadda26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_processes(path_chunk):\n",
    "    data = []\n",
    "    for event in iter_events(path_chunk):\n",
    "        if event[\"object\"] == \"PROCESS\" and event[\"action\"] == \"CREATE\":\n",
    "            if command_line := event.get(\"command_line\", \"\"):\n",
    "                data.append((event[\"objectID\"], 0, command_line))\n",
    "            elif image_path := event.get(\"image_path\", \"\"):\n",
    "                data.append((event[\"objectID\"], 10, image_path))\n",
    "            if parent_image_path := event.get(\"parent_image_path\", \"\"):\n",
    "                data.append((event[\"actorID\"], 10, parent_image_path))\n",
    "        else:\n",
    "            if image_path := event.get(\"image_path\", \"\"):\n",
    "                data.append((event[\"actorID\"], 10, image_path))\n",
    "\n",
    "    return filter_labels(pd.DataFrame(data=data, columns=[\"process_id\", \"importance\", \"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7fec9-f296-41a8-8071-60f887d65ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_known = pd.DataFrame()\n",
    "for fut in tqdm(client.map(label_processes, CHUNKS), total=len(CHUNKS)):\n",
    "    labels_known = filter_labels(pd.concat([labels_known, fut.result()], ignore_index=True))\n",
    "labels_known"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cadf7a-c11e-4bf3-a558-1e7009d6acd3",
   "metadata": {},
   "source": [
    "After all this, if any process is still without label, we give up and label it as `(unknown)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bbd01-a728-4457-9854-3cf015d6ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.Series(irow2process, name=\"process_id\").to_frame().merge(labels_known[[\"process_id\", \"label\"]], on=\"process_id\", how=\"left\").fillna(\"(unknown)\")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5526eabb-f2c3-4f72-84c9-e283d8d66ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels.loc[labels[\"label\"] == \"(unknown)\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230bbc9f-6c35-41ba-ad45-25024fd624d1",
   "metadata": {},
   "source": [
    "## Save for later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2321cb-9ed3-42da-b48b-5250ccb129bc",
   "metadata": {},
   "source": [
    "[Notebook 02](02%20Interactive%20visualization.ipynb) in this repository will cover the interactive visualization\n",
    "(was it a spoiler at this point?) of all these vectors data we generated.\n",
    "We write it all to disk so this notebook can read it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bc4b03-5aed-4d21-8d51-80e412646c84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(\"manifest.json\", \"wt\", encoding=\"utf-8\") as file:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"host\": str(HOST),\n",
    "            \"days\": [str(d) for d in DAYS]\n",
    "        },\n",
    "        file\n",
    "    )\n",
    "ss.save_npz(\"features.npz\", culled_matrix, compressed=True)\n",
    "np.savez_compressed(\"map2d.npz\", process_map=process_map)\n",
    "metadata_pruned.to_csv(\"metadata.csv.gz\", index=False, compression=\"gzip\")\n",
    "labels.to_csv(\"labels.csv.gz\", index=False, compression=\"gzip\")\n",
    "with gzip.open(\"col2token.pkl.gz\", \"wb\") as file:\n",
    "    cpkl.dump(col2token, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac951b4a-4d07-49d2-b71e-c7f29aff51e5",
   "metadata": {},
   "source": [
    "If one would rather use the data I canned into the Azure bucket instead of this one they just generated,\n",
    "they can delete or rename these three files:\n",
    "\n",
    "1. `manifest.json`\n",
    "1. `vectors.npz`\n",
    "1. `labels.csv.gz`\n",
    "1. `col2token.pkl.gz`\n",
    "\n",
    "Even altering just one will get the visualization notebook to fall back onto canned data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scipy 2023",
   "language": "python",
   "name": "scipy2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
